# Supervised Machine Learning

This repository contains code and explanations for different supervised machine learning algorithms that I have implemented. Each algorithm is implemented using Python. Some of the algorithms are implemented from scratch using only NumPy and Pandas. The code is well documented and easy to follow. And some algorithms are also implemented using scikit-learn.

## Algorithms

### Gradient Descent - Regression

The gradient descent algorithm is implemented for linear regression. It finds the best-fit line for a given dataset by minimizing the sum of the squared errors. The implementation uses the mean squared error (MSE) as the cost function.

### Gradient Descent - Logistic Regression

The gradient descent algorithm is implemented for logistic regression. It finds the best parameters for a logistic regression model by minimizing the log loss function. The implementation uses stochastic gradient descent (SGD) for optimization.

### Neural Nets - MLP

A multi-layer perceptron (MLP) is implemented using the Keras library. The model is trained on a binary classification task and achieves high accuracy on the test set.

### k-Nearest Neighbors

The k-nearest neighbors (k-NN) algorithm is implemented using scikit-learn. The algorithm finds the k nearest neighbors to a given data point and predicts the label based on the majority label among the neighbors.

### Decision/ Regression Trees

Decision trees and regression trees are implemented using scikit-learn. The algorithms learn a tree-based model from a given dataset and make predictions based on the learned tree.

### Ensemble Learning

Two ensemble learning algorithms, bagging and boosting, are implemented using scikit-learn. Bagging creates multiple models on random subsets of the data and combines their predictions, while boosting trains models iteratively and gives more weight to the misclassified examples.


