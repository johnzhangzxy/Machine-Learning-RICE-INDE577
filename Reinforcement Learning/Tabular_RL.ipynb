{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "---\n",
    "## Introduction \n",
    "\n",
    "Reinforcement learning is a type of machine learning that allows an agent to learn from its environment through trial and error, rather than being explicitly taught. The agent learns from its past experiences and tries to capture the best possible knowledge to make accurate business decisions. Reinforcement learning is different from supervised learning in a way that in supervised learning the training data has the answer key with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of a training dataset, it is bound to learn from its experience.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:735/1*bD2QuWCSVcnFH8j2iPMtbQ.png\" width=\"400\" >\n",
    "\n",
    "---\n",
    "\n",
    "## Reinforcement Learning Terminologies\n",
    "\n",
    "To explain in more detail, let's break it down:\n",
    "\n",
    "1. Agent: This is the \"learner\" or \"decision-maker\". In the tic-tac-toe example, the agent would be the program playing the game.\n",
    "\n",
    "2. Environment: Everything that the agent interacts with falls under this. In the tic-tac-toe example, this would be the game board.\n",
    "\n",
    "3. Actions: These are the set of all possible things the agent can do. For tic-tac-toe, an action would be placing a circle or a cross in a certain spot on the board.\n",
    "\n",
    "4. State: This is the current situation the agent finds itself in. The state of a tic-tac-toe game could be the current configuration of the game board.\n",
    "\n",
    "5. Reward: Feedback given to the agent after each action it takes. This could be positive (if the action was good) or negative (if the action was bad). For instance, in tic-tac-toe, winning the game might yield a reward of +1, losing might yield -1, and a draw might yield 0.\n",
    "\n",
    "The objective of the agent is to learn a policy, which is a strategy that dictates which action to take in each state so as to maximize the sum of rewards over time. This is achieved via exploration (trying out new actions to see their effect) and exploitation (using actions that are known to yield high rewards).\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Tabular Methods - tic-tac-toe\n",
    "\n",
    "I will Apply tabular methods for solving a well known RL problems, tic-tac-toe, like the Q-learning algorithm. In this context, a table is used to store the value (the expected future reward) of each action in each state. Initially, this table is initialized arbitrarily, and then it's updated iteratively based on the rewards the agent receives as it interacts with the environment. Over time, the agent learns to choose the action with the highest value in each state, thereby maximizing its total reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the environment\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.state = ' ' * 9\n",
    "        self.player = 'X'\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i, spot in enumerate(self.state) if spot == ' ']\n",
    "\n",
    "    def update_state(self, action):\n",
    "        state = list(self.state)\n",
    "        state[action] = self.player\n",
    "        self.state = ''.join(state)\n",
    "        self.player = 'O' if self.player == 'X' else 'X'\n",
    "\n",
    "    def is_done(self):\n",
    "        winning_spots = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        for spot in winning_spots:\n",
    "            if self.state[spot[0]] == self.state[spot[1]] == self.state[spot[2]] != ' ':\n",
    "                return True\n",
    "        return ' ' not in self.state\n",
    "\n",
    "    def reward(self):\n",
    "        if self.is_done():\n",
    "            if self.player == 'O':\n",
    "                return -1\n",
    "            elif self.player == 'X':\n",
    "                return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon=0.2, alpha=0.3, gamma=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(available_actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, action) for action in available_actions]\n",
    "            return available_actions[np.argmax(q_values)]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_actions):\n",
    "        old_q_value = self.get_q_value(state, action)\n",
    "        if next_actions:\n",
    "            max_next_q_value = max([self.get_q_value(next_state, next_action) for next_action in next_actions])\n",
    "        else:\n",
    "            max_next_q_value = 0\n",
    "        self.q_table[(state, action)] = (1 - self.alpha) * old_q_value + self.alpha * (reward + self.gamma * max_next_q_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 10000\n",
    "\n",
    "agent = QLearningAgent()\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    game = TicTacToe()\n",
    "    state = game.state\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state, game.available_actions())\n",
    "        game.update_state(action)\n",
    "        reward = game.reward()\n",
    "        next_state = game.state\n",
    "        done = game.is_done()\n",
    "        agent.learn(state, action, reward, next_state, game.available_actions())\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent, human_starts=True):\n",
    "    game = TicTacToe()\n",
    "    if human_starts:\n",
    "        game.player = 'O'\n",
    "\n",
    "    while not game.is_done():\n",
    "        print(game.state[:3] + \"\\n\" + game.state[3:6] + \"\\n\" + game.state[6:])\n",
    "        if game.player == 'X':\n",
    "            action = agent.choose_action(game.state, game.available_actions())\n",
    "            game.update_state(action)\n",
    "        else:\n",
    "            action = int(input(\"Choose your action (0-8): \"))\n",
    "            while action not in game.available_actions():\n",
    "                print(\"Invalid action.\")\n",
    "                action = int(input(\"Choose your action (0-8): \"))\n",
    "            game.update_state(action)\n",
    "\n",
    "    if game.reward() == 1:\n",
    "        print(\"The agent has won!\")\n",
    "    elif game.reward() == -1:\n",
    "        print(\"You've won!\")\n",
    "    else:\n",
    "        print(\"It's a draw!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "   \n",
      "   \n",
      "X  \n",
      "   \n",
      "   \n",
      "X O\n",
      "   \n",
      "   \n",
      "XXO\n",
      "   \n",
      "   \n",
      "XXO\n",
      "O  \n",
      "   \n",
      "XXO\n",
      "O X\n",
      "   \n",
      "XXO\n",
      "OOX\n",
      "   \n",
      "XXO\n",
      "OOX\n",
      " X \n",
      "The agent has won!\n"
     ]
    }
   ],
   "source": [
    "play(agent, human_starts=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johnz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
